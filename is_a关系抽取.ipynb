{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1b38FyurMzXCzsIYQ2EMmcaMa_9M685kd",
      "authorship_tag": "ABX9TyO6/e+uJoWDyOscJdeHaTp7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/XJS924/30-Days-Of-Python/blob/master/is_a%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/projects/is_a_extract/\n",
        "!zip is_a.zip found.txt not_found.txt"
      ],
      "metadata": {
        "id": "SMKkOPjXYh-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDSLfw03Yoeq"
      },
      "outputs": [],
      "source": [
        "!pip3 uninstall keras-nightly tensorboard-plugin-wit\n",
        "!pip3 install keras==2.2.4 tensorflow-gpu==1.15 bert4keras h5py==2.10.0 toolkit4nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 文本分类"
      ],
      "metadata": {
        "id": "A29jBbJRy0ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from bert4keras.backend import keras, set_gelu\n",
        "from bert4keras.tokenizers import Tokenizer\n",
        "from bert4keras.models import build_transformer_model\n",
        "from bert4keras.optimizers import Adam, extend_with_piecewise_linear_lr\n",
        "from bert4keras.snippets import sequence_padding, DataGenerator\n",
        "from bert4keras.snippets import open\n",
        "from keras.layers import Lambda, Dense\n",
        "\n",
        "set_gelu('tanh')  # 切换gelu版本\n",
        "\n",
        "num_classes = 2\n",
        "maxlen = 36\n",
        "batch_size = 256\n",
        "path = '/content/drive/MyDrive/models/roberta_zh_base_ckpt/'\n",
        "config_path = path+ 'bert_config.json'\n",
        "checkpoint_path = path+'bert_model.ckpt'\n",
        "dict_path = path+'vocab.txt'\n",
        "\n",
        "\n",
        "def load_data(filename):\n",
        "    \"\"\"加载数据\n",
        "    单条格式：(文本, 标签id)\n",
        "    \"\"\"\n",
        "    D = []\n",
        "    num = 0\n",
        "    with open(filename, encoding='utf-8') as f:\n",
        "      for l in f:\n",
        "        if num==0:\n",
        "          pass\n",
        "        else:\n",
        "          text, label = l.strip().split('\\t')\n",
        "          D.append((text, int(label)))\n",
        "        num+=1\n",
        "    return D\n",
        "\n",
        "\n",
        "# 加载数据集\n",
        "train_data = load_data(\"/content/drive/MyDrive/data/queryGoodsPropertyTask/train_new.txt\")\n",
        "# valid_data = load_data('datasets/sentiment/sentiment.valid.data')\n",
        "test_data = load_data('/content/drive/MyDrive/data/queryGoodsPropertyTask/test_new.txt')\n",
        "\n",
        "# 建立分词器\n",
        "tokenizer = Tokenizer(dict_path, do_lower_case=True)\n",
        "\n",
        "\n",
        "class data_generator(DataGenerator):\n",
        "    \"\"\"数据生成器\n",
        "    \"\"\"\n",
        "    def __iter__(self, random=False):\n",
        "        batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
        "        for is_end, (text, label) in self.sample(random):\n",
        "            token_ids, segment_ids = tokenizer.encode(text, maxlen=maxlen)\n",
        "            batch_token_ids.append(token_ids)\n",
        "            batch_segment_ids.append(segment_ids)\n",
        "            batch_labels.append([label])\n",
        "            if len(batch_token_ids) == self.batch_size or is_end:\n",
        "                batch_token_ids = sequence_padding(batch_token_ids)\n",
        "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
        "                batch_labels = sequence_padding(batch_labels)\n",
        "                yield [batch_token_ids, batch_segment_ids], batch_labels\n",
        "                batch_token_ids, batch_segment_ids, batch_labels = [], [], []\n",
        "\n",
        "\n",
        "# 加载预训练模型\n",
        "bert = build_transformer_model(\n",
        "    config_path=config_path,\n",
        "    checkpoint_path=checkpoint_path,\n",
        "    # model='albert',\n",
        "    return_keras_model=False,\n",
        ")\n",
        "\n",
        "output = Lambda(lambda x: x[:, 0], name='CLS-token')(bert.model.output)\n",
        "output = Dense(\n",
        "    units=num_classes,\n",
        "    activation='softmax',\n",
        "    kernel_initializer=bert.initializer\n",
        ")(output)\n",
        "\n",
        "model = keras.models.Model(bert.model.input, output)\n",
        "model.summary()\n",
        "\n",
        "# 派生为带分段线性学习率的优化器。\n",
        "# 其中name参数可选，但最好填入，以区分不同的派生优化器。\n",
        "AdamLR = extend_with_piecewise_linear_lr(Adam, name='AdamLR')\n",
        "\n",
        "model.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    # optimizer=Adam(1e-5),  # 用足够小的学习率\n",
        "    optimizer=AdamLR(lr=1e-4, lr_schedule={\n",
        "        1000: 1,\n",
        "        2000: 0.1\n",
        "    }),\n",
        "    metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "# 转换数据集\n",
        "train_generator = data_generator(train_data, batch_size)\n",
        "# valid_generator = data_generator(valid_data, batch_size)\n",
        "test_generator = data_generator(test_data, batch_size)\n",
        "\n",
        "\n",
        "def evaluate(data):\n",
        "    total, right = 0., 0.\n",
        "    for x_true, y_true in data:\n",
        "        y_pred = model.predict(x_true).argmax(axis=1)\n",
        "        y_true = y_true[:, 0]\n",
        "        total += len(y_true)\n",
        "        right += (y_true == y_pred).sum()\n",
        "    return right / total\n",
        "\n",
        "\n",
        "class Evaluator(keras.callbacks.Callback):\n",
        "    \"\"\"评估与保存\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.best_val_acc = 0.\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        val_acc = evaluate(test_generator)\n",
        "        if val_acc > self.best_val_acc:\n",
        "            self.best_val_acc = val_acc\n",
        "            model.save_weights('best_model.weights')\n",
        "        test_acc = evaluate(test_generator)\n",
        "        print(\n",
        "            u'val_acc: %.5f, best_val_acc: %.5f, test_acc: %.5f\\n' %\n",
        "            (val_acc, self.best_val_acc, test_acc)\n",
        "        )"
      ],
      "metadata": {
        "id": "i3TyVh_ay0F0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if __name__ == '__main__':\n",
        "\n",
        "evaluator = Evaluator()\n",
        "\n",
        "model.fit(\n",
        "    train_generator.forfit(),\n",
        "    steps_per_epoch=len(train_generator),\n",
        "    epochs=10,\n",
        "    callbacks=[evaluator]\n",
        ")\n",
        "\n",
        "model.load_weights('best_model.weights')\n",
        "print(u'final test acc: %05f\\n' % (evaluate(test_generator)))\n",
        "\n",
        "# else:\n",
        "\n",
        "#   model.load_weights('best_model.weights')"
      ],
      "metadata": {
        "id": "OuZY26jw7-1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from bert4keras.backend import keras, K\n",
        "from bert4keras.backend import sparse_multilabel_categorical_crossentropy\n",
        "from bert4keras.tokenizers import Tokenizer\n",
        "from bert4keras.layers import EfficientGlobalPointer as GlobalPointer\n",
        "from bert4keras.models import build_transformer_model\n",
        "from bert4keras.optimizers import Adam, extend_with_exponential_moving_average\n",
        "from bert4keras.snippets import sequence_padding, DataGenerator\n",
        "from bert4keras.snippets import open, to_array\n",
        "from tqdm import tqdm\n",
        "\n",
        "maxlen = 128\n",
        "batch_size = 64\n",
        "config_path = '/root/kg/bert/chinese_L-12_H-768_A-12/bert_config.json'\n",
        "checkpoint_path = '/root/kg/bert/chinese_L-12_H-768_A-12/bert_model.ckpt'\n",
        "dict_path = '/root/kg/bert/chinese_L-12_H-768_A-12/vocab.txt'\n",
        "\n",
        "\n",
        "def load_data(filename):\n",
        "    \"\"\"加载数据\n",
        "    单条格式：{'text': text, 'spo_list': [(s, p, o)]}\n",
        "    \"\"\"\n",
        "    D = []\n",
        "    with open(filename, encoding='utf-8') as f:\n",
        "        for l in f:\n",
        "            l = json.loads(l)\n",
        "            D.append({\n",
        "                'text': l['text'],\n",
        "                'spo_list': [(spo['subject'], spo['predicate'], spo['object'])\n",
        "                             for spo in l['spo_list']]\n",
        "            })\n",
        "    return D\n",
        "\n",
        "\n",
        "# 加载数据集\n",
        "train_data = load_data('/root/kg/datasets/train_data.json')\n",
        "valid_data = load_data('/root/kg/datasets/dev_data.json')\n",
        "predicate2id, id2predicate = {}, {}\n",
        "\n",
        "with open('/root/kg/datasets/all_50_schemas') as f:\n",
        "    for l in f:\n",
        "        l = json.loads(l)\n",
        "        if l['predicate'] not in predicate2id:\n",
        "            id2predicate[len(predicate2id)] = l['predicate']\n",
        "            predicate2id[l['predicate']] = len(predicate2id)\n",
        "\n",
        "# 建立分词器\n",
        "tokenizer = Tokenizer(dict_path, do_lower_case=True)\n",
        "\n",
        "\n",
        "def search(pattern, sequence):\n",
        "    \"\"\"从sequence中寻找子串pattern\n",
        "    如果找到，返回第一个下标；否则返回-1。\n",
        "    \"\"\"\n",
        "    n = len(pattern)\n",
        "    for i in range(len(sequence)):\n",
        "        if sequence[i:i + n] == pattern:\n",
        "            return i\n",
        "    return -1\n",
        "\n",
        "\n",
        "class data_generator(DataGenerator):\n",
        "    \"\"\"数据生成器\n",
        "    \"\"\"\n",
        "    def __iter__(self, random=False):\n",
        "        batch_token_ids, batch_segment_ids = [], []\n",
        "        batch_entity_labels, batch_head_labels, batch_tail_labels = [], [], []\n",
        "        for is_end, d in self.sample(random):\n",
        "            token_ids, segment_ids = tokenizer.encode(d['text'], maxlen=maxlen)\n",
        "            # 整理三元组 {(s, o, p)}\n",
        "            spoes = set()\n",
        "            for s, p, o in d['spo_list']:\n",
        "                s = tokenizer.encode(s)[0][1:-1]\n",
        "                p = predicate2id[p]\n",
        "                o = tokenizer.encode(o)[0][1:-1]\n",
        "                sh = search(s, token_ids)\n",
        "                oh = search(o, token_ids)\n",
        "                if sh != -1 and oh != -1:\n",
        "                    spoes.add((sh, sh + len(s) - 1, p, oh, oh + len(o) - 1))\n",
        "            # 构建标签\n",
        "            entity_labels = [set() for _ in range(2)]\n",
        "            head_labels = [set() for _ in range(len(predicate2id))]\n",
        "            tail_labels = [set() for _ in range(len(predicate2id))]\n",
        "            for sh, st, p, oh, ot in spoes:\n",
        "                entity_labels[0].add((sh, st))\n",
        "                entity_labels[1].add((oh, ot))\n",
        "                head_labels[p].add((sh, oh))\n",
        "                tail_labels[p].add((st, ot))\n",
        "            for label in entity_labels + head_labels + tail_labels:\n",
        "                if not label:  # 至少要有一个标签\n",
        "                    label.add((0, 0))  # 如果没有则用0填充\n",
        "            entity_labels = sequence_padding([list(l) for l in entity_labels])\n",
        "            head_labels = sequence_padding([list(l) for l in head_labels])\n",
        "            tail_labels = sequence_padding([list(l) for l in tail_labels])\n",
        "            # 构建batch\n",
        "            batch_token_ids.append(token_ids)\n",
        "            batch_segment_ids.append(segment_ids)\n",
        "            batch_entity_labels.append(entity_labels)\n",
        "            batch_head_labels.append(head_labels)\n",
        "            batch_tail_labels.append(tail_labels)\n",
        "            if len(batch_token_ids) == self.batch_size or is_end:\n",
        "                batch_token_ids = sequence_padding(batch_token_ids)\n",
        "                batch_segment_ids = sequence_padding(batch_segment_ids)\n",
        "                batch_entity_labels = sequence_padding(\n",
        "                    batch_entity_labels, seq_dims=2\n",
        "                )\n",
        "                batch_head_labels = sequence_padding(\n",
        "                    batch_head_labels, seq_dims=2\n",
        "                )\n",
        "                batch_tail_labels = sequence_padding(\n",
        "                    batch_tail_labels, seq_dims=2\n",
        "                )\n",
        "                yield [batch_token_ids, batch_segment_ids], [\n",
        "                    batch_entity_labels, batch_head_labels, batch_tail_labels\n",
        "                ]\n",
        "                batch_token_ids, batch_segment_ids = [], []\n",
        "                batch_entity_labels, batch_head_labels, batch_tail_labels = [], [], []\n",
        "\n",
        "\n",
        "def globalpointer_crossentropy(y_true, y_pred):\n",
        "    \"\"\"给GlobalPointer设计的交叉熵\n",
        "    \"\"\"\n",
        "    shape = K.shape(y_pred)\n",
        "    y_true = y_true[..., 0] * K.cast(shape[2], K.floatx()) + y_true[..., 1]\n",
        "    y_pred = K.reshape(y_pred, (shape[0], -1, K.prod(shape[2:])))\n",
        "    loss = sparse_multilabel_categorical_crossentropy(y_true, y_pred, True)\n",
        "    return K.mean(K.sum(loss, axis=1))\n",
        "\n",
        "\n",
        "# 加载预训练模型\n",
        "base = build_transformer_model(\n",
        "    config_path=config_path,\n",
        "    checkpoint_path=checkpoint_path,\n",
        "    return_keras_model=False\n",
        ")\n",
        "\n",
        "# 预测结果\n",
        "entity_output = GlobalPointer(heads=2, head_size=64)(base.model.output)\n",
        "head_output = GlobalPointer(\n",
        "    heads=len(predicate2id), head_size=64, RoPE=False, tril_mask=False\n",
        ")(base.model.output)\n",
        "tail_output = GlobalPointer(\n",
        "    heads=len(predicate2id), head_size=64, RoPE=False, tril_mask=False\n",
        ")(base.model.output)\n",
        "outputs = [entity_output, head_output, tail_output]\n",
        "\n",
        "# 构建模型\n",
        "AdamEMA = extend_with_exponential_moving_average(Adam, name='AdamEMA')\n",
        "optimizer = AdamEMA(learning_rate=1e-5)\n",
        "model = keras.models.Model(base.model.inputs, outputs)\n",
        "model.compile(loss=globalpointer_crossentropy, optimizer=optimizer)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "def extract_spoes(text, threshold=0):\n",
        "    \"\"\"抽取输入text所包含的三元组\n",
        "    \"\"\"\n",
        "    tokens = tokenizer.tokenize(text, maxlen=maxlen)\n",
        "    mapping = tokenizer.rematch(text, tokens)\n",
        "    token_ids, segment_ids = tokenizer.encode(text, maxlen=maxlen)\n",
        "    token_ids, segment_ids = to_array([token_ids], [segment_ids])\n",
        "    outputs = model.predict([token_ids, segment_ids])\n",
        "    outputs = [o[0] for o in outputs]\n",
        "    # 抽取subject和object\n",
        "    subjects, objects = set(), set()\n",
        "    outputs[0][:, [0, -1]] -= np.inf\n",
        "    outputs[0][:, :, [0, -1]] -= np.inf\n",
        "    for l, h, t in zip(*np.where(outputs[0] > threshold)):\n",
        "        if l == 0:\n",
        "            subjects.add((h, t))\n",
        "        else:\n",
        "            objects.add((h, t))\n",
        "    # 识别对应的predicate\n",
        "    spoes = set()\n",
        "    for sh, st in subjects:\n",
        "        for oh, ot in objects:\n",
        "            p1s = np.where(outputs[1][:, sh, oh] > threshold)[0]\n",
        "            p2s = np.where(outputs[2][:, st, ot] > threshold)[0]\n",
        "            ps = set(p1s) & set(p2s)\n",
        "            for p in ps:\n",
        "                spoes.add((\n",
        "                    text[mapping[sh][0]:mapping[st][-1] + 1], id2predicate[p],\n",
        "                    text[mapping[oh][0]:mapping[ot][-1] + 1]\n",
        "                ))\n",
        "    return list(spoes)\n",
        "\n",
        "\n",
        "class SPO(tuple):\n",
        "    \"\"\"用来存三元组的类\n",
        "    表现跟tuple基本一致，只是重写了 __hash__ 和 __eq__ 方法，\n",
        "    使得在判断两个三元组是否等价时容错性更好。\n",
        "    \"\"\"\n",
        "    def __init__(self, spo):\n",
        "        self.spox = (\n",
        "            tuple(tokenizer.tokenize(spo[0])),\n",
        "            spo[1],\n",
        "            tuple(tokenizer.tokenize(spo[2])),\n",
        "        )\n",
        "\n",
        "    def __hash__(self):\n",
        "        return self.spox.__hash__()\n",
        "\n",
        "    def __eq__(self, spo):\n",
        "        return self.spox == spo.spox\n",
        "\n",
        "\n",
        "def evaluate(data):\n",
        "    \"\"\"评估函数，计算f1、precision、recall\n",
        "    \"\"\"\n",
        "    X, Y, Z = 1e-10, 1e-10, 1e-10\n",
        "    f = open('dev_pred.json', 'w', encoding='utf-8')\n",
        "    pbar = tqdm()\n",
        "    for d in data:\n",
        "        R = set([SPO(spo) for spo in extract_spoes(d['text'])])\n",
        "        T = set([SPO(spo) for spo in d['spo_list']])\n",
        "        X += len(R & T)\n",
        "        Y += len(R)\n",
        "        Z += len(T)\n",
        "        f1, precision, recall = 2 * X / (Y + Z), X / Y, X / Z\n",
        "        pbar.update()\n",
        "        pbar.set_description(\n",
        "            'f1: %.5f, precision: %.5f, recall: %.5f' % (f1, precision, recall)\n",
        "        )\n",
        "        s = json.dumps({\n",
        "            'text': d['text'],\n",
        "            'spo_list': list(T),\n",
        "            'spo_list_pred': list(R),\n",
        "            'new': list(R - T),\n",
        "            'lack': list(T - R),\n",
        "        },\n",
        "                       ensure_ascii=False,\n",
        "                       indent=4)\n",
        "        f.write(s + '\\n')\n",
        "    pbar.close()\n",
        "    f.close()\n",
        "    return f1, precision, recall\n",
        "\n",
        "\n",
        "class Evaluator(keras.callbacks.Callback):\n",
        "    \"\"\"评估与保存\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.best_val_f1 = 0.\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        optimizer.apply_ema_weights()\n",
        "        f1, precision, recall = evaluate(valid_data)\n",
        "        if f1 >= self.best_val_f1:\n",
        "            self.best_val_f1 = f1\n",
        "            model.save_weights('best_model.weights')\n",
        "        optimizer.reset_old_weights()\n",
        "        print(\n",
        "            'f1: %.5f, precision: %.5f, recall: %.5f, best f1: %.5f\\n' %\n",
        "            (f1, precision, recall, self.best_val_f1)\n",
        "        )\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    train_generator = data_generator(train_data, batch_size)\n",
        "    evaluator = Evaluator()\n",
        "\n",
        "    model.fit(\n",
        "        train_generator.forfit(),\n",
        "        steps_per_epoch=len(train_generator),\n",
        "        epochs=20,\n",
        "        callbacks=[evaluator]\n",
        "    )\n",
        "\n",
        "else:\n",
        "\n",
        "    model.load_weights('best_model.weights')"
      ],
      "metadata": {
        "id": "rl5Cb5S7YsJ2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}